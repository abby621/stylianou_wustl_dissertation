% !TEX root = thesis.tex

\chapter{Visualizing Deep Similarity Networks}
\label{ch:8}

While convolutional neural networks have become a transformative tool for many image analysis tasks, it is still common in the literature to describe these deep learning approaches as ''black boxes''. To address these concerns, there have been substantial efforts to understand and visualize the features of classification networks~\cite{bau2017network,netdissect2017,visualization_techreport,RTC16,deepInside,szegedy2015going,tolias2016rmac,ZeilerF13,cam,scenecnn_iclr15}. However, much less work has focused on visualizing and understanding similarity networks, which learn an embedding that maps similar examples to nearby vectors in feature space and dissimilar examples to be far apart~\cite{song2016deep,yi2014deep}. 
%Similarity learning has been used to support face matching~\cite{FIXME}, person re-identification~\cite{FIXME}, and object tracking~\cite{FIXME}.

\begin{figure}[t]
    \setlength\tabcolsep{1.25pt}
    \centering
        \begin{tabular}{c|ccc}
             Query & \multicolumn{3}{|c}{Top Matches}  \\
             \includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/landmarks/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/landmarks/1.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/landmarks/2.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/landmarks/3.png}}
             \\
             \includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/faces/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/faces/1.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/faces/2.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/faces/3.png}}
             \\
             \includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/traffickcam/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/traffickcam/1.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/traffickcam/2.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeight]{figures/chapter7/frontPage/traffickcam/3.png}}
        \end{tabular}
     \caption{Our approach to visualizing the embeddings generated by deep similarity networks calculates the contribution of each pixel location to the overall similarity between two images. We evaluate our approach on a variety of problem domains and network architectures.}
     \label{fig:frontPage}
\end{figure}

Our approach highlights the image regions that contributed the most to the overall similarity between two images. Figure~\ref{fig:frontPage} shows example visualizations for the top image retrieval results from three different application domains (Google Landmarks~\cite{googleLandmarks}, VGG-Faces~\cite{vggfaces}, and Traffickcam Hotel Rooms~\cite{aipr2015}). Each row of the figure shows a query image and the three most similar database images returned from a network trained for the respective task. The heatmap overlay shows the relative spatial contribution of each image to the similarity score with the query.
 
Our approach aligns with the recent trend toward explainability for learning-based tasks and extends recent work in visualizing classification networks to the case of similarity networks. Our specific contributions include:
\begin{itemize}
\itemsep0em 
    \item a novel visualization approach for similarity networks;
    \item an analysis of the effect of training and ``late-stage'' pooling strategies for similarity networks; and,
    \item an approach to using similarity visualizations to support object- and region-based image retrieval.
\end{itemize}

%We demonstrate these contributions across several different problem domains, including indoor and outdoor scene recognition, as well as facial recognition.



% \item From our NIJ Proposal, other image retrieval citations:~\cite{baatz2012large,chen2011city,crandall2009mapping,schindler2007city,torii2013visual,zamir2010accurate,hays2008im2gps,jacobs07geolocate,zhou2014recognizing}

\section{Background}
Visualizations provide a way to better understand the learning process underlying deep neural networks. Much of the work in this area focuses on visualizations for classification networks and not similarity networks. While networks used for each type of problem share many similarities, the differences in the output (i.e., sparse vs. dense feature vectors) is significant, requiring new methods for visualizing similarity networks.

\paragraph{CNN Visualization}
Previous work on CNN visualizations can be broadly categorized by
the depth of the portion of the networked being visualized. Some methods provide visualizations that highlight the inner layer activations~\cite{netdissect2017,visualization_techreport,scenecnn_iclr15}. A majority of the work targets the output layer to produce visualizations which seek to explain
why classification networks output a particular label for an image. These include approaches that mask off parts of the input images and provide a visual quantification of the impact on the output classification~\cite{ZeilerF13}. Another approach generates saliency maps, which represent which pixels in an image contributed to a particular output node~\cite{deepInside}. There has been work that generates class activation maps, which map an output back to the last convolutional layer in the network by weighting the filters in that layer by the weights between the final pooling layer and the output feature~\cite{cam}. Inception~\cite{szegedy2015going}, which hallucinates images that activate a particular class from random noise, can also serve as visualization tool to provide insight into the learning process. 

\paragraph{Similarity Learning}
Much of the work in similarity learning with deep neural networks focuses on learning better similarity functions using, for example, pairwise losses~\cite{sun2014deep,wang2014learning,yi2014deep}, triplet losses~\cite{HermansBeyer2017Arxiv,schroff2015facenet,song2016deep,ustinova2016learning}, and direct embedding~\cite{Proxy}. Compared to the efforts toward understanding classification networks, there has been much less work in visualizing and analyzing similarity networks. One method visualizes the similarity of single filters from the different convolutional layers of an embedding network~\cite{Ahmed_2015_CVPR}. Another method computes image similarity as the inner product between the normalized elements of a final max pooling layer and produces a visualization with bounding boxes around highly active regions for the ten features that contribute most to the similarity of a pair of images~\cite{RTC16,tolias2016rmac}. 

Visualizing a few features is effective for networks that tend to be sparse, but in Section~\ref{sec:FeatureImportance} we show that in embedding networks the similarity tends to be explained by a large number of features.  This motivates our approach to visualize how all features affect the similarity score.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/chapter7/process.png}
    \caption{Our approach considers similarity networks with a final convolutional layer, $\boldsymbol{\alpha}$, followed by a pooling operation which produces output features, $\boldsymbol{\beta}$. Similarity between two images is measured as the dot product of these output features after normalization.  Factoring this value produces visualizations that highlight how much each region of the image contributes to the similarity.}
    \label{fig:visApproach}
\end{figure*}
\section{Visualization Approach}
\label{sec:model}

Networks used in similarity learning broadly consist of: (1) a convolutional portion, (2) a "flattening" operation (usually max or global average pooling), and (3) a fully-connected portion. A recent study covering a number of image retrieval tasks, however, suggests that the best generalization performance is obtained using the output from the layer immediately after the pooling operation~\cite{vo2018generalization}. Our approach is applicable to networks of this structure, including popular models such as the Resnet~\cite{resnet} and VGG~\cite{vggfaces} network architectures.  

Given an input image, $I$, and a trained similarity network, our approach relies on the activations of the layers before and after the pooling operation. Let $\boldsymbol{\alpha}$ represent the $K \times K \times C$ tensor of the last convolutional layer, where $K$ represents the length and width (usually equal) and $C$ represents the number of filters. Let $\boldsymbol{\beta}$ represent the $C$-dimensional vector after the pooling operation for an image, as shown in Figure~\ref{fig:visApproach}. In similarity learning, the dot product of these normalized feature vectors is a widely-used similarity function~\cite{bell2015learning,Proxy,schroff2015facenet,sun2014deep,wang2014learning,yi2014deep}, so the similarity of two images $I^{(i)},I^{(j)}$ can be written as:
\begin{equation}
s(\boldsymbol{\beta}^{(i)},\boldsymbol{\beta}^{(j)}) = \frac{\boldsymbol{\beta}^{(i)} \cdot \boldsymbol{\beta}^{(j)}}{\norm{\boldsymbol{\beta}^{(i)}} \norm{\boldsymbol{\beta}^{(j)}}}
\label{eq:cosineSim}
\end{equation}
Our visualization approach results in spatial similarity maps, where the overall similarity between two image feature vectors is spatially decomposed to highlight the contribution of image regions to the overall pairwise similarity, as shown in Figure~\ref{fig:visApproach}. Computing the similarity maps depends on the flattening operation between the convolutional portion of the network and the output feature. Max pooling and global average pooling are the most commonly applied operations at this stage in modern networks. We show how our similarity maps are computed for each case.

\subsection{Average Pooling}
For networks which employ average pooling as the flattening operation, the output feature, $\boldsymbol{\beta}$, is:
\begin{equation} 
\boldsymbol{\beta} = \frac{1}{K^2}\sum_{x,y} \boldsymbol{\alpha}_{(x,y)}
\label{eq:avgPool}
\end{equation}
\noindent where $\boldsymbol{\alpha}_{(x,y)}$ represents the $C$-dimensional slice of $\boldsymbol{\alpha}$  at spatial location $(x,y)$. The similarity of images $I^{(i)}$ and $I^{(j)}$ can be directly decomposed spatially, by substituting $\boldsymbol{\beta}^{(i)}$ in Equation~\ref{eq:cosineSim} with Equation~\ref{eq:avgPool}:
\begin{align}
    s(\boldsymbol{\beta}^{(i)},\boldsymbol{\beta}^{(j)}) &= \frac{\boldsymbol{\beta}^{(i)}\cdot\boldsymbol{\beta}^{(j)}}{\norm{\boldsymbol{\beta}^{(i)}} \norm{\boldsymbol{\beta}^{(j)}}}\nonumber\\
    &=
    \frac{\frac{1}{K^2}\left(\boldsymbol{\alpha}^{(i)}_{(1,1)} + \ldots + \boldsymbol{\alpha}^{(i)}_{(K,K)}\right)  \cdot \boldsymbol{\beta^{(j)}}}{\norm{\boldsymbol{\beta}^{(i)}} \norm{\boldsymbol{\beta}^{(j)}}}\nonumber\\
    &= \frac{\boldsymbol{\alpha}^{(i)}_{(1,1)} \cdot \boldsymbol{\beta}^{(j)} + \ldots + \boldsymbol{\alpha}^{(i)}_{(K,K)} \cdot \boldsymbol{\beta}^{(j)}
    }{Z}
\end{align}
\noindent where $Z$ is the normalizing factor $K^2 \norm{\boldsymbol{\beta}^{(i)}} \norm{\boldsymbol{\beta}^{(j)}}$.

These terms can be rearranged spatially and visualized as a heat-map to show the relative contribution of each part of the image to the overall similarity. Symmetrically, the similarity can be decomposed to highlight the contribution of the other image in the pair to the overall similarity, as shown on the right side of Figure~\ref{fig:visApproach}.
% \begin{align}
%  s(\boldsymbol{\beta}^{(i)},\boldsymbol{\beta}^{(j)}) &=
%  \frac{\boldsymbol{\alpha}^{(j)}_{(1,1)} \cdot \boldsymbol{\beta}^{(i)} + \ldots + \boldsymbol{\alpha}^{(j)}_{(K,K)} \cdot \boldsymbol{\beta}^{(i)}}{Z}.
% \end{align}

\subsection{Max Pooling}
With a modification, the approach can also accommodate networks that use max pooling as the flattening operation. In max pooling, each element of an output vector $\boldsymbol{\beta}$ is equal to the max value of the activation of its corresponding filter in the last convolutional layer:
\begin{equation}
\boldsymbol{\beta} = \max_{x,y} \boldsymbol{\alpha}_{(x,y)}
\label{eq:maxPool}
\end{equation}
\noindent Unlike average pooling, where each of the composite components contribute equally to the output feature, decomposing max pooled features requires an additional step. For a max pooled feature, $\boldsymbol{\beta}$, we construct a surrogate tensor, $\boldsymbol{\hat{\alpha}}$, for the convolutional portion as follows:
\begin{equation}
    \hat{\alpha}_{(x,y,c)} = \begin{cases}
    0 & \text{if } \alpha_{(x,y,c)} \neq \beta_{(c)} \\
    \frac{\alpha_{(x,y,c)}}{N_{(c)}} & \text{if } \alpha_{(x,y,c)} = \beta_{(c)} \\
    \end{cases}
\end{equation}
\noindent where $N_{(c)}$ represents the number of spatial locations equal to the maximum value for filter $c$. That is, for each filter, we assign the maximum value to the location that generated it (divided evenly in cases of ties), and zero otherwise. This gives the following formulation for the spatial similarity decomposition in the case of max pooling:
\begin{align}
     s(\boldsymbol{\beta}^{(i)},\boldsymbol{\beta}^{(j)}) &=
    \frac{\boldsymbol{\hat{\alpha}}^{(i)}_{(1,1)} \cdot \boldsymbol{\hat{\beta}}^{(j)} + \ldots + \boldsymbol{\hat{\alpha}}^{(i)}_{(K,K)} \cdot \boldsymbol{\hat{\beta}}^{(j)}}{\norm{\boldsymbol{\beta}^{(i)}}\norm{\boldsymbol{\beta}^{(j)}}}
\end{align}
Similar to the case for average pooling, similarity maps can be computed in either direction for a pair of images. 

We scale the heatmaps using bilinear interpolation and blend them with the original image to show which parts of the images contribute to the similarity scores.

\section{Results}
Similarity networks trained for three different problem domains are used to test the approach. Except where noted, we use the following network architectures and output features. For the Google Landmarks~\cite{googleLandmarks} and TraffickCam Hotel Rooms~\cite{aipr2015} datasets, we fine-tune a Resnet-50~\cite{resnet} network from pre-trained ILSVRC weights~\cite{ILSVRC15} using the combinatorial variant of triplet loss described in~\cite{HermansBeyer2017Arxiv}. For the VGG-Faces dataset, we use the VGG-Faces network trained on the VGG-Faces2 dataset~\cite{vggfaces,vggface2}. For each of the networks, we use the layer immediately after the pooling operation as our output features (2048-D for Resnet-50, and 512-D for VGG-Faces).

\begin{figure}
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/chapter7/contribByFeature.png}
    \caption{The plot shows the average contribution of the top\==K components of the feature vectors to the similarity score between pairs of images from the same class using the pre-trained VGG-Faces dataset. The top 10 features (the number of features visualized in prior work, and identified in this plot by a red dot), account for less than 30\% of the similarity score between two images, motivating our attempt to visualize all features.}
    \label{fig:contribByFeature}
\end{figure}

\subsection{Feature Importance}
\label{sec:FeatureImportance}
Prior work in understanding similarity networks focuses on either a few filters or few regions that contribute most to the similarity between a pair of images~\cite{Ahmed_2015_CVPR,RTC16,tolias2016rmac}. Our visualization approach, by comparison, summarizes the contribution of every feature to the similarity between a pair of images. 

In the following experiment, we demonstrate that for similarity networks, the top few most important components only represent a small fraction of the overall image similarity.  Figure~\ref{fig:contribByFeature} shows the average contribution of the first $k$ components for 1000 randomly sampled pairs of images from the same class using the pre-trained VGG-Faces network. The top 10 features (the number of features visualized in prior work, and identified in this plot by a red dot) contribute less than 30\% of the overall similarity score. This suggests that, unlike classification networks which output sparse feature vectors, understanding the output of similarity networks requires a visualization approach that explains more than only a few features at once. Our approach to visualizing similarity networks incorporates all of the feature vector components and calculates the contribution of each pixel location to the overall similarity between two images.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{.48\columnwidth}
        \centering
        \begin{tabular}{cc}
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/landmarks/1_im.png} &  
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/landmarks/2_im.png}\\
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/landmarks/1_hm.png} &  
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/landmarks/2_hm.png}\\
        \end{tabular}
    \end{subfigure}
    \begin{subfigure}[b]{.48\columnwidth}
        \centering
        \begin{tabular}{cc}
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/faces/1_im.png} &  
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/faces/2_im.png}\\
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/faces/1_hm.png} &  
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/faces/2_hm.png}
        \end{tabular}
    \end{subfigure}
    \\\vspace{20px}
    \begin{subfigure}[b]{.48\columnwidth}
        \centering
        \begin{tabular}{cc}
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/traffickcam/1_im.png} &
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/traffickcam/2_im.png}\\
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/traffickcam/1_hm.png} &
            \includegraphics[width=.45\columnwidth]{figures/chapter7/clarification/traffickcam/2_hm.png}
        \end{tabular}
    \end{subfigure}
    \caption{Visualizations to understand image similarity. (Top-left) For two images of the same landmark, the visualization highlights the building in the background in left image, but the foreground in the right. (Top-right) For two images of the same person, the nose and mouth region are highlighted. (Bottom) For two images of rooms from different hotels, the visualization highlights the similar light fixtures mounted to the headboard.}
    \label{fig:clarification}
\end{figure}


\subsection{Visualizing Pairwise Similarity}
Figure~\ref{fig:clarification} shows pairs of images that produced high similarity scores. In the top pair of images from the Google Landmarks dataset, the viewpoints are quite different, but the visualization approach highlights the specific building that the network identified as being similar. This building is in the foreground of one of the images, but hidden in the background of the other. The middle pair of images are of the same gentleman in the VGG-Faces dataset. the visualization highlights his lower facial features. The final pair of images is from different hotels in the TraffickCam Hotel Rooms dataset. The visualization highlights that both rooms have similar light fixtures mounted to the headboard. These examples demonstrate the ability of the visualization approach in explaining why a network produces similar embeddings for a pair of images, even in cases where that may not be readily apparent to a human observer looking at the images.

\begin{figure*}
    \centering
    \setlength\tabcolsep{1.25pt}
    \begin{subfigure}[b]{.47\textwidth}
        \begin{tabular}{c|ccc}
             Query & \multicolumn{3}{|c}{Top Matches}  \\
             \includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/1.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/2.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/3.png}}
             \\
             Initial
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/pretrained/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/pretrained/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/pretrained/3.png}}
             \\
             5k iterations
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/4999/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/4999/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/4999/3.png}}
             \\
             25k iterations
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/24999/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/24999/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/24999/3.png}}
             \\
             \ 50k iterations \
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/49999/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/49999/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/landmarks/49999/3.png}}
        \end{tabular}
       % \caption{Google Landmarks}
    \end{subfigure}
    \\\vspace{1cm}
    \begin{subfigure}[b]{.47\textwidth}
    \begin{tabular}{c|ccc}
             Query & \multicolumn{3}{|c}{Top Matches}  \\
             \includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/1.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/2.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/3.png}}
             \\
             Initial
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/pretrained/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/pretrained/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/pretrained/3.png}}
             \\
             5k iterations
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/4999/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/4999/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/4999/3.png}}
             \\
             50k iterations
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/49999/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/49999/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/49999/3.png}}
             \\
             100k iterations
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/99999/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/99999/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/over_time/traffickcam/99999/3.png}}
        \end{tabular}
   %     \caption{TraffickCam Hotel Rooms}
    \end{subfigure}
    \caption{Each figure shows visualizations from networks pre-trained on ImageNet and fine-tuned on Google Landmarks (top) and TraffickCam Hotel Rooms (bottom) during the training process.}
    \label{fig:overTime}
\end{figure*}


\setlength\tabcolsep{2pt}
\begin{figure*}
    \centering
    \begin{subfigure}[b]{\columnwidth}
    \begin{tabular}{c|ccc}
             Query & \multicolumn{3}{|c}{Top Matches}  \\
             \includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/1.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/2.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/3.png}}
             \\
             From Scratch
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/fromScratch/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/fromScratch/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/fromScratch/3.png}}
             \\
             Fine-tuned
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/finetuned/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/finetuned/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/landmarks/finetuned/3.png}}
        \end{tabular}
     %   \caption{Google Landmarks}
    \end{subfigure}
    \\ \vspace{1cm}
    \begin{subfigure}[b]{\columnwidth}
    \begin{tabular}{c|ccc}
            Query & \multicolumn{3}{|c}{Top Matches}  \\
             \includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/1.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/2.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/3.png}}
             \\
             From Scratch
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/fromScratch/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/fromScratch/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/fromScratch/3.png}}
             \\
             Fine-tuned
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/finetuned/1.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/finetuned/2.png}}
             &
             \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/finetuning_vs_fromScratch/traffickcam/finetuned/3.png}}
        \end{tabular}
     %   \caption{TraffickCam Hotel Room Dataset}
    \end{subfigure}
    \caption{Fine-tuning vs. Training from Scratch. The visualization highlights that, regardless of the initialization, the networks converge to similar representations.}
    \label{fig:finetuning_vs_fromScratch}
\end{figure*}

\setlength\tabcolsep{2pt}
\begin{figure*}
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \begin{tabular}{c|ccc}
             Query & \multicolumn{3}{|c}{Top Matches}  \\
             \includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/13/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/13/1.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/13/2.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/13/3.png}}
             \\
             \includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/35/query.png}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/35/1.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/35/2.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/35/3.png}}
             \\
             \includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/37/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/37/1.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/37/2.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/avg/37/3.png}}
        \end{tabular}
        \caption{Average Pooling}
    \end{subfigure}
    \begin{subfigure}[b]{\columnwidth}
           \begin{tabular}{c|ccc}
           Query & \multicolumn{3}{|c}{Top Matches}  \\
             \includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/13/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/13/1.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/13/2.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/13/3.png}}
             \\
             \includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/35/query.png}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/35/1.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/35/2.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/35/3.png}}
             \\
             \includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/37/query.png}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/37/1.png}}
             &
             \fcolorbox{green}{green}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/37/2.png}}
             &
             \fcolorbox{red}{red}{\includegraphics[height=\tableImHeightTwo]{figures/chapter7/avg_vs_max_pooling/max/37/3.png}}
        \end{tabular}
        \caption{Max Pooling}
    \end{subfigure}
    \caption{Average vs. Max Pooling. For the same VGG-Faces network architecture, these visualizations show the pairwise similarity for models trained with average pooling and max pooling.}
    \label{fig:avg_vs_max_pooling}
\end{figure*}

\subsection{Similarity Learning During Training}
Figure~\ref{fig:overTime} shows the visualization for a query image and its top 3 most similar images during the training process. For the Google Landmarks dataset, we see that even by 5,000 iterations, the network has largely learned that it is 
the skyline that makes this scene recognizable. In subsequent iterations, the network refines the similarity metric and focuses on more specific regions, such as the buildings in the scene. On the TraffickCam Hotel Rooms dataset, on the other hand, the network takes longer to learn a similarity embedding. At 5,000 iterations, the network has not yet focused on specific elements of the hotel rooms. By 50,000 iterations, it becomes clear that the headboard is the relevant part of this particular set of images, and by 100,000 iterations, the network appears to be refining that focus. These examples demonstrate the utility of the visualization in understanding \textbf{\textit{when}} a network has learned a useful similarity metric, in addition to understanding what components of a scene the network has learned to focus on.

Another consideration when training similarity networks is whether to train from scratch or fine-tune from pre-trained weights. Figure~\ref{fig:finetuning_vs_fromScratch} shows the top three results for a query image, the similarity visualizations when trained from scratch, and when fine-tuned from pre-trained weights. In the examples from the Google Landmarks and TraffickCam Hotel Rooms dataset, we see that both the fine-tuned network and the network trained from scratch converged to similar encodings of similarity (e.g., both the fine-tuned network and network trained from scratch highlight the building facade in the Google Landmarks scene and the headboard in the TraffickCam hotel). These results suggest
that both approaches converge to features that encode the same important elements of the scenes and that it is reasonable to fine-tune from pre-trained weights (even from a fairly dissimilar task, such as a classification task trained on ILSVRC).

\subsection{Average vs. Max Pooling}

As described in Section~\ref{sec:model}, the visualization approach is applicable to networks with either average and max pooling at the end of the convolutional portion of the network. Figure~\ref{fig:avg_vs_max_pooling} shows the comparison between two VGG-Faces networks, one trained with average pooling and one with max pooling. 
For the same image pairs, the embeddings highlight different regions. For example, 
the average pooling network focuses on glasses in the first query image, while the max pooling network focuses more on eyebrow shape. Additionally, the regions of similarity are larger in the average pooling network compared to the max pooling network. This is reasonable as all of the regions contribute to output embedding in average pooling, but not max pooling. 


\setlength\tabcolsep{2.5pt}
\renewcommand{\arraystretch}{3.25}
\begin{figure}
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \begin{tabular}{ccccc}
            Class 1 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/1/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/1/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/1/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/1/4.png}}
            \\
            Class 2 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/2/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/2/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/2/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/2/4.png}}
            \\
            Class 3 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/3/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/3/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/3/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/landmarks/3/4.png}}
        \end{tabular}
        \caption{Google Landmarks}
    \end{subfigure}
    \begin{subfigure}[b]{\columnwidth}
        \begin{tabular}{ccccc}
            Class 1 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/1/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/1/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/1/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/1/4.png}}
            \\
            Class 2 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/3/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/3/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/3/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/3/4.png}}
            \\
            Class 3 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/2/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/2/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/2/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/faces/2/4.png}}
        \end{tabular}
        \caption{VGG-Faces}
    \end{subfigure}
    \begin{subfigure}[b]{\columnwidth}
        \begin{tabular}{ccccc}
            Class 1 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/1/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/1/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/1/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/1/4.png}}
            \\
            Class 2 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/3/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/3/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/3/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/3/4.png}}
            \\
            Class 3 &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/2/1.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/2/2.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/2/3.png}} &
            \raisebox{-.5\height}{\includegraphics[height=\tableImHeight]{figures/chapter7/what_makes_a_class/traffickcam/2/4.png}}
        \end{tabular}
        \caption{TraffickCam Hotel Rooms}
    \end{subfigure}
    \caption{Each row shows the regions of the images most representative of the class membership.}
    \label{fig:whatMakesAClass}
\end{figure}

\subsection{Class Similarity}
Using our method, we can discover the most representative components of a class of images. This is a natural extension of class activation maps for classification networks, which visualize the components of an image contribute the most to a particular output label. We generate class activation maps for a given image by summing the pairwise similarity maps with the other images in the same class.
Figure~\ref{fig:whatMakesAClass} shows class similarity visualization for a selection of images from each of our datasets. The visualizations highlight the
portions of the image that most contribute to the similarity of the output feature to those of the images in the same class. For example, in Class 1 of the Google Landmarks dataset, the clock on the building's facade is the most important part in each example image; in Class 2 of the VGG-Faces dataset, the nose and lips are most important; and in Class 3 of the TraffickCam Hotel Rooms dataset, the headboard is most associated with the hotel identity.

